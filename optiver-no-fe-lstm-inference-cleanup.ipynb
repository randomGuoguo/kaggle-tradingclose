{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":168.614017,"end_time":"2023-10-30T00:47:50.42774","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-10-30T00:45:01.813723","version":"2.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":7116023,"sourceType":"datasetVersion","datasetId":4103835}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#mi3 \n#mi3-lgb\n#mi3-nn-files \n#mi3-nn-submit \n#mi3-nn-online-submit\n#mi3-ensemble-online-submit\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom scipy.stats import hmean\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nimport itertools\nimport pickle\nimport joblib\nfrom itertools import combinations\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, GaussianNoise\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.experimental import CosineDecay\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import concatenate,Dropout\nimport pickle\nfrom tensorflow.keras.models import load_model\nimport os \nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import Huber\nfrom tensorflow.keras.metrics import MeanAbsoluteError\nfrom tensorflow.keras.callbacks import Callback\nimport random\n# from catboost import CatBoostRegressor, Pool\n#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n\nkaggle              = True\nis_inference        = True\nload_models         = True\nrun_pipeline        = False\ntrain_models        = False\n\nis_rnn              = True   #3\nonline_learning     = True\n\n\n#public-validation\n# dates_train = [0,390]\n# dates_test = [391,480]\n\n#full-inference\ndates_train = [0,480]\ndates_test = [-1,-1]\n\n\nnum_models ={'rnn':3} \n\n\nif kaggle:\n    train_path = \"/kaggle/input/optiver-trading-at-the-close/train.csv\"\n    models_path = \"/kaggle/input/optiver-3x-244-rnn-decay7-models/\"\nelse:\n    models_path = \"optiver-inference/\"\n    train_path = \"train.csv\"\n\nif dates_train[1]!=480:\n    models_path = \"vals-rnn/\"\n#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n","metadata":{"papermill":{"duration":13.419877,"end_time":"2023-10-30T00:45:18.598788","exception":false,"start_time":"2023-10-30T00:45:05.178911","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:04:58.978269Z","iopub.execute_input":"2023-12-18T01:04:58.978681Z","iopub.status.idle":"2023-12-18T01:04:58.992059Z","shell.execute_reply.started":"2023-12-18T01:04:58.978648Z","shell.execute_reply":"2023-12-18T01:04:58.990807Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nrnn_ep         = 1\nrnn_lr         = 0.001\nrnn_bs         = 2**12\nwindow_size    = 3","metadata":{"execution":{"iopub.status.busy":"2023-12-18T01:04:58.994480Z","iopub.execute_input":"2023-12-18T01:04:58.994906Z","iopub.status.idle":"2023-12-18T01:04:59.010657Z","shell.execute_reply.started":"2023-12-18T01:04:58.994874Z","shell.execute_reply":"2023-12-18T01:04:59.007180Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#@title read train\n\ncolab = False\nif colab:\n  from google.colab import drive\n  drive.mount('/content/drive')\n  train_path = '/content/drive/My Drive/optiver/train.csv'\n  models_path = \"/content/drive/My Drive/optiver/\"\nelse:\n    if not kaggle:\n        from public_timeseries_testing_util import MockApi\n\npd.set_option('mode.chained_assignment', None)\n# dtypes = {\n#     'stock_id' : np.uint8,\n#     'date_id' : np.uint16,\n#     # 'seconds_in_bucket' : np.uint16,\n#     # 'imbalance_buy_sell_flag' : np.int8,\n#     'time_id' : np.uint16,\n# }\n\ndef calculate_stock_return(x):\n    return ((x / x.shift(6)).shift(-6) - 1) * 10_000\n\n\n\n\ndef convert_price_cols_float32(df):\n\n    # Columns containing 'price'\n    price_columns = [col for col in df.columns if 'price' in col]\n    df[price_columns] = df[price_columns].astype('float32')\n\n    # Columns containing 'wap'\n    wap_columns = [col for col in df.columns if 'wap' in col]\n    df[wap_columns] = df[wap_columns].astype('float32')\n\n    return df\n\ntrain = pd.read_csv(train_path).drop(['row_id', 'time_id'], axis = 1)\nnan_count = train['target'].isna().sum()\nprint(f\"The 'target' column has {nan_count} NaN values.\")\n\ntwo_out = False\nif two_out:\n    train[\"stock_return\"] = train.groupby([\"stock_id\", \"date_id\"])[\"wap\"].transform(calculate_stock_return)\n    train['stock_return'].fillna(train['stock_return'].median(), inplace=True)\n\ntarget_median = train['target'].median()\ntrain['target'].fillna(target_median, inplace=True)\n\n\nprint(f\"converting prices columns to float32 values.\")\ntrain = convert_price_cols_float32(train)\n# ----------------------------- Reading train data -------------------------","metadata":{"papermill":{"duration":17.742575,"end_time":"2023-10-30T00:45:36.347807","exception":false,"start_time":"2023-10-30T00:45:18.605232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:04:59.012248Z","iopub.execute_input":"2023-12-18T01:04:59.013388Z","iopub.status.idle":"2023-12-18T01:05:11.777298Z","shell.execute_reply.started":"2023-12-18T01:04:59.013342Z","shell.execute_reply":"2023-12-18T01:05:11.775997Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"The 'target' column has 88 NaN values.\nconverting prices columns to float32 values.\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title functions\n\ndef split_by_date(df, dates):\n\n    df_start, df_end = dates\n    df = df[(df['date_id'] >= df_start) & (df['date_id'] <=df_end)].reset_index(drop=True)\n\n    return df\n\n\n\ndef lag_function(df, columns_to_lag, numbers_of_days_to_lag):\n\n    df_indexed = df.set_index(['stock_id', 'seconds_in_bucket', 'date_id'])\n    \n    for column_to_lag in columns_to_lag:\n        for number_days_to_lag in numbers_of_days_to_lag:\n            df_indexed[f'lag{number_days_to_lag}_{column_to_lag}'] = df_indexed.groupby(level=['stock_id', 'seconds_in_bucket'])[column_to_lag].shift(number_days_to_lag)\n    \n    df_indexed.reset_index(inplace=True)\n    \n    return df_indexed\n\n\n\ndef save_pickle(data, file_path):\n \n    # Create the directory if it doesn't exist\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Save the pickle file\n    with open(file_path, 'wb') as file:\n        pickle.dump(data, file)\n\n    print(f\"Data saved to {file_path}\")\n    #example: save_pickle(all_data, 'k8/all_data.pkl')\n\ndef load_pickle(file_path):\n\n    # Load and return the data from the pickle file\n    if os.path.exists(file_path):\n        with open(file_path, 'rb') as file:\n            data = pickle.load(file)\n        return data\n    else:\n        raise FileNotFoundError(f\"No such file: {file_path}\")\n\n    #example: loaded_data = load_pickle('k8/all_data.pkl')\n\n\n\n\n\n","metadata":{"papermill":{"duration":0.048164,"end_time":"2023-10-30T00:45:36.402382","exception":false,"start_time":"2023-10-30T00:45:36.354218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:05:11.779789Z","iopub.execute_input":"2023-12-18T01:05:11.780147Z","iopub.status.idle":"2023-12-18T01:05:11.793756Z","shell.execute_reply.started":"2023-12-18T01:05:11.780116Z","shell.execute_reply":"2023-12-18T01:05:11.792448Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#@title pipeline\n\nraw_cols          = ['imbalance_size','matched_size','bid_size','ask_size','reference_price','far_price','near_price','bid_price','ask_price','wap','imbalance_buy_sell_flag'] \n\ncolumns_prices    = ['reference_price','far_price','near_price','bid_price','ask_price','wap']\ncolumns_4prices   = ['reference_price','bid_price','ask_price','wap']\n\ncolumns_sizes     = ['imbalance_size','matched_size','bid_size','ask_size']\ncolumns_flag      = ['imbalance_buy_sell_flag'] \n\n\ntarget_lags         = [1, 2, 3]\n\n\ndef feature_pipeline(df):\n    \n    if df.empty:\n        return pd.DataFrame()\n        \n\n    print(f\"lagging target column for {len(target_lags)} lags.\")\n    df = lag_function(df, ['target'], target_lags)\n\n\n    print(\"Done...\")\n    \n    return df","metadata":{"papermill":{"duration":0.020161,"end_time":"2023-10-30T00:45:36.45631","exception":false,"start_time":"2023-10-30T00:45:36.436149","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:05:11.795680Z","iopub.execute_input":"2023-12-18T01:05:11.796037Z","iopub.status.idle":"2023-12-18T01:05:11.809028Z","shell.execute_reply.started":"2023-12-18T01:05:11.795992Z","shell.execute_reply":"2023-12-18T01:05:11.807763Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n\ndef make_predictions(models, X_test,model = 'nn'):\n    if model == 'nn':\n        all_predictions = [model.predict(X_test, batch_size=16384) for model in models]\n    if model == 'lgb' or model == 'xgb' or model == 'cat':\n        all_predictions = [model.predict(X_test) for model in models]\n    prediction = np.mean(all_predictions, axis=0)\n    return prediction\n\ndef set_all_seeds(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nclass BestScoresCallback(Callback):\n    def __init__(self):\n        super().__init__()\n        self.best_train_loss = float('inf')\n        self.best_val_loss = float('inf')\n\n    def on_epoch_end(self, epoch, logs=None):\n        train_loss = logs.get('loss', float('inf'))\n        val_loss = logs.get('val_loss', float('inf'))\n\n        if train_loss < self.best_train_loss:\n            self.best_train_loss = train_loss\n        if val_loss < self.best_val_loss:\n            self.best_val_loss = val_loss\n\n    def on_train_end(self, logs=None):\n        print(f\"Best training loss: {self.best_train_loss}, Best validation loss: {self.best_val_loss}\")\n\n\n\n\n\n","metadata":{"papermill":{"duration":0.03576,"end_time":"2023-10-30T00:45:36.517275","exception":false,"start_time":"2023-10-30T00:45:36.481515","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:05:11.810678Z","iopub.execute_input":"2023-12-18T01:05:11.810984Z","iopub.status.idle":"2023-12-18T01:05:11.826616Z","shell.execute_reply.started":"2023-12-18T01:05:11.810958Z","shell.execute_reply":"2023-12-18T01:05:11.825442Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#@title RNN second pass\n\ndef precompute_sequences(stock_data, window_size, rnn_numerical_features, rnn_categorical_features):\n    # Convert DataFrame columns to NumPy arrays\n    stock_data_num = stock_data[rnn_numerical_features].values\n    stock_data_cat = stock_data[rnn_categorical_features].values\n\n    # Pre-compute all sequences\n    all_sequences_num = [stock_data_num[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n    all_sequences_cat = [stock_data_cat[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n\n    # Add padding if necessary\n    padded_sequences_num = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_num]\n    padded_sequences_cat = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_cat]\n\n    # Combine numerical and categorical features\n    combined_sequences = np.array([np.concatenate([num, cat], axis=-1) \n                                   for num, cat in zip(padded_sequences_num, padded_sequences_cat)])\n\n    # Extract targets\n    targets = stock_data['target'].values\n\n    return combined_sequences, targets\n\ndef get_sequence(precomputed_data, time_step):\n    combined_sequences, targets = precomputed_data\n    return combined_sequences[time_step], targets[time_step]\n\n\n\ndef create_batches(data, window_size, rnn_numerical_features, rnn_categorical_features, max_time_steps=55):\n    \n    grouped = data.groupby(['stock_id', 'date_id'])\n    all_batches = []\n    all_targets = []\n\n    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n        # Precompute sequences for the current group\n        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n\n        # Initialize containers for group sequences and targets\n        group_sequences = []\n        group_targets = []\n\n        # Iterate over the time steps and retrieve precomputed sequences\n        for time_step in range(max_time_steps):\n            sequence, target = get_sequence(precomputed_data, time_step)\n            if sequence.size > 0: \n                group_sequences.append(sequence)\n                group_targets.append(target)\n\n        # Extend the main batches with the group's sequences and targets\n        all_batches.extend(group_sequences)\n        all_targets.extend(group_targets)\n\n    return all_batches, all_targets\n\n\n\n\ndef compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features):\n    # Convert DataFrame columns to NumPy arrays\n    stock_data_num      = group[rnn_numerical_features].values\n    stock_data_cat      = group[rnn_categorical_features].values\n    stock_data_target   = group['target'].values\n\n    # Find the index of the target second\n    target_index = len(group) - 1\n\n    # Extract the sequence for the target index\n    sequence_num = stock_data_num[max(0, target_index - window_size + 1):target_index + 1]\n    sequence_cat = stock_data_cat[max(0, target_index - window_size + 1):target_index + 1]\n\n    # Add padding if necessary\n    padded_sequence_num = np.pad(sequence_num, ((window_size - len(sequence_num), 0), (0, 0)), 'constant')\n    padded_sequence_cat = np.pad(sequence_cat, ((window_size - len(sequence_cat), 0), (0, 0)), 'constant')\n\n    # Combine numerical and categorical features\n    combined_sequence = np.concatenate([padded_sequence_num, padded_sequence_cat], axis=-1)\n\n    # Extract target\n    target = stock_data_target[-1]\n\n    return combined_sequence, target\n\n\ndef create_last_batches(data, window_size, rnn_numerical_features, rnn_categorical_features):\n    \n    grouped = data.groupby(['stock_id'])\n    all_batches = []\n    all_targets = []\n\n    for _, group in grouped:\n        # Compute the sequence for the last data point in the current group\n        last_sequence, last_target = compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features)\n\n        # Check if the sequence is valid (i.e., not empty)\n        if last_sequence.size > 0: \n            all_batches.append(last_sequence)\n            all_targets.append(last_target)\n\n    return all_batches, all_targets\n\n\n\n\n\ndef second_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size, is_inference=False):\n    # Check if the DataFrame is empty\n    global rnn_scaler,rnn_medians\n    \n    if df.empty:\n        return None, None\n\n    # Work on a copy of the DataFrame to avoid changing the original df\n    df_copy = df.copy()\n\n    # Standard scaling for numerical features\n    if is_inference:\n        df_copy.fillna(rnn_medians, inplace=True)\n        df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n        \n    # Preprocess Data\n    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n    df_copy['imbalance_buy_sell_flag'] += 1 \n\n    if is_inference:\n        df_copy_batches, df_copy_targets = create_last_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n    else:\n        df_copy_batches, df_copy_targets = create_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n\n    df_copy_batches = np.array(df_copy_batches)\n    df_copy_targets = np.array(df_copy_targets)\n\n\n    return df_copy_batches, df_copy_targets\n\n\ndef online_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size):\n    # Check if the DataFrame is empty\n    global rnn_scaler,rnn_medians\n    \n    if df.empty:\n        return None, None\n\n    # Work on a copy of the DataFrame to avoid changing the original df\n    df_copy = df.copy()\n\n    # Standard scaling for numerical features\n    df_copy.fillna(rnn_medians, inplace=True)\n    df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n        \n    # Preprocess Data\n    df_copy['seconds_in_bucket'] = df_copy['seconds_in_bucket'] / 10\n    df_copy['imbalance_buy_sell_flag'] += 1 \n\n\n    grouped = df_copy.groupby(['stock_id'])\n    all_batches = []\n    all_targets = []\n\n    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n        # Precompute sequences for the current group\n        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n\n        # Initialize containers for group sequences and targets\n        group_sequences = []\n        group_targets = []\n\n        # Iterate over the time steps and retrieve precomputed sequences\n        for time_step in range(55):\n            sequence, target = get_sequence(precomputed_data, time_step)\n            if sequence.size > 0: \n                group_sequences.append(sequence)\n                group_targets.append(target)\n\n        # Extend the main batches with the group's sequences and targets\n        all_batches.extend(group_sequences)\n        all_targets.extend(group_targets)\n\n    df_batches = np.array(all_batches)\n    df_targets = np.array(all_targets)\n\n\n    return df_batches, df_targets\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T01:05:11.828238Z","iopub.execute_input":"2023-12-18T01:05:11.828578Z","iopub.status.idle":"2023-12-18T01:05:11.856761Z","shell.execute_reply.started":"2023-12-18T01:05:11.828548Z","shell.execute_reply":"2023-12-18T01:05:11.855456Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#@title RNN model\nfrom tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import ZeroPadding1D\n\n\ndef create_rnn_model_with_residual(window_size, numerical_features, initial_learning_rate=0.001):\n    \n    categorical_features = 'seconds_in_bucket'\n    categorical_uniques  = { 'seconds_in_bucket' : 55}\n    embedding_dim        = {'seconds_in_bucket' : 10}\n\n    input_layer = Input(shape=(window_size, len(numerical_features) + 1), name=\"combined_input\")\n\n    # Split the input into numerical and categorical parts\n    numerical_input = Lambda(lambda x: x[:, :, :-1], name=\"numerical_part\")(input_layer)\n    categorical_input = Lambda(lambda x: x[:, :, -1:], name=\"categorical_part\")(input_layer)\n\n    # Function to create a difference layer for a given lag\n    def create_difference_layer(lag):\n        return Lambda(lambda x: x[:, lag:, :] - x[:, :-lag, :], name=f\"difference_layer_lag{lag}\")\n\n    # List to store all difference layers\n    difference_layers = []\n\n    # Create difference layers for each lag\n    for lag in range(1, window_size):\n        diff_layer = create_difference_layer(lag)(numerical_input)\n        padding = ZeroPadding1D(padding=(lag, 0))(diff_layer)  # Add padding to the beginning of the sequence\n        difference_layers.append(padding)\n\n    \n\n    combined_diff_layer = Concatenate(name=\"combined_difference_layer\")(difference_layers) \n    \n    enhanced_numerical_input = Concatenate(name=\"enhanced_numerical_input\")([numerical_input, combined_diff_layer])\n\n#     concat_input = Concatenate(name=\"concatenated_input\")([enhanced_numerical_input, categorical_input])\n\n    # Embedding for categorical part\n    vocab_size, embedding_dim = categorical_uniques[categorical_features], embedding_dim[categorical_features]\n    embedding = Embedding(vocab_size, embedding_dim, input_length=window_size)(categorical_input)\n    embedding = Reshape((window_size, -1))(embedding)\n\n\n  \n    # Concatenate numerical input and embedding\n    lstm_input = concatenate([enhanced_numerical_input, embedding], axis=-1)\n\n    # Initialize a list to hold the outputs of each LSTM layer\n#     lstm_outputs = []\n\n    # First LSTM layer\n    lstml = LSTM(64, return_sequences=False)(lstm_input)\n    lstml = BatchNormalization()(lstml)\n    lstml = Dropout(0.3)(lstml)\n    \n    dense_output = lstml\n    dense_sizes = [512, 256, 128, 64, 32]\n    do_ratio = 0.3\n    for size in dense_sizes:\n        dense_output = Dense(size, activation='swish')(dense_output)\n        dense_output = BatchNormalization()(dense_output)\n        dense_output = Dropout(do_ratio)(dense_output)\n\n    # Output layer\n    output = Dense(1, name='output_layer')(dense_output)\n\n    # Learning rate schedule\n    lr_schedule = ExponentialDecay(\n        initial_learning_rate=initial_learning_rate,\n        decay_steps=10000,\n        decay_rate=0.7,\n        staircase=True)\n\n    # Create and compile the model\n    model = Model(inputs=input_layer, outputs=output)\n    optimizer = Adam(learning_rate=lr_schedule)\n\n    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T01:05:11.859272Z","iopub.execute_input":"2023-12-18T01:05:11.859675Z","iopub.status.idle":"2023-12-18T01:05:11.874251Z","shell.execute_reply.started":"2023-12-18T01:05:11.859644Z","shell.execute_reply":"2023-12-18T01:05:11.873152Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#@title runnig pipeline\n\nexcluded_columns = ['row_id', 'date_id', 'time_id', 'target','stock_return']  \n\nif run_pipeline:\n\n    train_eng = feature_pipeline(train)\n    \n    \n\n    if is_rnn:\n        excluded_columns = excluded_columns + ['stock_id']\n\n        features = [col for col in train_eng.columns if col not in excluded_columns]\n        rnn_categorical_features =  ['seconds_in_bucket']\n        rnn_numerical_features = [feat for feat in features if feat not in rnn_categorical_features]\n        print(\"we have {} numerical and {} categorical\".format(len(rnn_numerical_features),len(rnn_categorical_features)))\n\n\n        rnn_scaler = StandardScaler()\n        rnn_medians = train_eng.median()\n        \n        train_eng.fillna(rnn_medians, inplace=True)\n        train_eng[rnn_numerical_features] = rnn_scaler.fit_transform(train_eng[rnn_numerical_features])\n\n    \n        rnn_all_data = {\n            \"rnn_scaler\": rnn_scaler,\n            \"rnn_medians\": rnn_medians,\n            \"rnn_categorical_features\": rnn_categorical_features,\n            \"rnn_numerical_features\": rnn_numerical_features\n        }\n\n        save_pickle(rnn_all_data, f'{models_path}rnn_all_data.pkl')\n        print(\"Pipline Done!\")\n\n\n    train_data       = split_by_date(train_eng, dates_train)\n    test_data        = split_by_date(train_eng, dates_test)\n    print(\"number of dates in train = {} , number of dates in test {}\".format (train_data['date_id'].nunique(),test_data['date_id'].nunique()))\n\n    cleaning = False\n    if cleaning:\n        import gc\n        #del train\n        del train_eng\n        gc.collect()\n    \n\n","metadata":{"papermill":{"duration":0.404747,"end_time":"2023-10-30T00:45:36.928334","exception":false,"start_time":"2023-10-30T00:45:36.523587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:05:11.876840Z","iopub.execute_input":"2023-12-18T01:05:11.877242Z","iopub.status.idle":"2023-12-18T01:05:18.681957Z","shell.execute_reply.started":"2023-12-18T01:05:11.877210Z","shell.execute_reply":"2023-12-18T01:05:18.680831Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"lagging target column for 3 lags.\nDone...\nwe have 14 numerical and 1 categorical\nData saved to vals-rnn/rnn_all_data.pkl\nPipline Done!\nnumber of dates in train = 6 , number of dates in test 6\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title TPU\ntry:\n    # Create a TPUClusterResolver\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    # Connect to the TPU cluster\n    tf.config.experimental_connect_to_cluster(tpu)\n    # Initialize the TPU system\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # Create a TPUStrategy for distributed training\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    tpu_strategy = None  # No TPU found\n","metadata":{"papermill":{"duration":0.01349,"end_time":"2023-10-30T00:45:36.967612","exception":false,"start_time":"2023-10-30T00:45:36.954122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:05:18.683585Z","iopub.execute_input":"2023-12-18T01:05:18.684235Z","iopub.status.idle":"2023-12-18T01:05:18.690888Z","shell.execute_reply.started":"2023-12-18T01:05:18.684192Z","shell.execute_reply":"2023-12-18T01:05:18.689622Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#@title prepare train models\nif train_models:\n\n    if test_data.empty:\n        print(\"--------------test data is empty so adjusting the last date for test-----------------\")\n        test_data = train_data.query(\"date_id == 480\").copy()\n\n    if is_rnn:\n\n        train_batches, train_targets = second_pass_for_rnn(train_data, rnn_numerical_features, rnn_categorical_features, window_size)\n        test_batches, test_targets  = second_pass_for_rnn(test_data, rnn_numerical_features, rnn_categorical_features, window_size)\n        print(f\"train batches shape:{train_batches.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T01:05:18.692235Z","iopub.execute_input":"2023-12-18T01:05:18.692572Z","iopub.status.idle":"2023-12-18T01:05:35.950709Z","shell.execute_reply.started":"2023-12-18T01:05:18.692541Z","shell.execute_reply":"2023-12-18T01:05:35.949712Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Processing groups: 100%|██████████| 1200/1200 [00:08<00:00, 143.82it/s]\nProcessing groups: 100%|██████████| 1200/1200 [00:08<00:00, 143.37it/s]","output_type":"stream"},{"name":"stdout","text":"train batches shape:(66000, 3, 15)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title training models\n# train_data = flatten_outliers(train_data, 'target', lower_quantile=0.01, upper_quantile=0.99)\nregular = True\nif train_models:\n        \n        directory = os.path.dirname(models_path)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n   \n        if is_rnn:\n\n            if True or tpu_strategy:\n                # with tpu_strategy.scope():\n\n                    rnn_models = []\n                    for i in range(num_models['rnn']):\n\n                        print(f\"Training rnn model {i+1} out of {num_models['rnn']} with seed {42+i}\")\n                        print(\"---------------------------------------\")\n                        set_all_seeds(42+i)\n                        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n\n                        rnn_model = create_rnn_model_with_residual(window_size, rnn_numerical_features, initial_learning_rate=rnn_lr)\n                        history = rnn_model.fit(train_batches, train_targets, validation_data=(test_batches, test_targets), epochs=rnn_ep, batch_size=rnn_bs, callbacks=[early_stopping])\n                        print(\"---------------------------------------\")\n                        rnn_model.save(f'{models_path}rnn_model_seed_{i}.h5')\n                        rnn_models.append(rnn_model)\n\n                    predictions =  make_predictions(rnn_models, test_batches,model = 'nn')                        \n                    print(f\"Ensemble Mean Absolute Error: {mean_absolute_error(test_targets, predictions):.4f}\")\n","metadata":{"papermill":{"duration":0.017103,"end_time":"2023-10-30T00:45:36.990986","exception":false,"start_time":"2023-10-30T00:45:36.973883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:05:35.952237Z","iopub.execute_input":"2023-12-18T01:05:35.952579Z","iopub.status.idle":"2023-12-18T01:06:11.475187Z","shell.execute_reply.started":"2023-12-18T01:05:35.952549Z","shell.execute_reply":"2023-12-18T01:06:11.474021Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Training rnn model 1 out of 3 with seed 42\n---------------------------------------\n17/17 [==============================] - 9s 234ms/step - loss: 5.9861 - val_loss: 5.9041\n---------------------------------------\nTraining rnn model 2 out of 3 with seed 43\n---------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"17/17 [==============================] - 9s 232ms/step - loss: 6.0085 - val_loss: 5.9041\n---------------------------------------\nTraining rnn model 3 out of 3 with seed 44\n---------------------------------------\n17/17 [==============================] - 9s 240ms/step - loss: 6.0085 - val_loss: 5.9048\n---------------------------------------\n5/5 [==============================] - 1s 123ms/step\n5/5 [==============================] - 1s 132ms/step\n5/5 [==============================] - 1s 133ms/step\nEnsemble Mean Absolute Error: 5.9042\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title timeseries functions\n\ndef clean_format(df):\n    df['target'] = df['target'].astype('float64')\n    for feat in ['stock_id','date_id','seconds_in_bucket']:\n        df[feat] = df[feat].astype('int64')\n    return df\n\ndef clean_up(df):\n    df = df[['stock_id','revealed_date_id','seconds_in_bucket','revealed_target']].rename(columns={'revealed_date_id': 'date_id', 'revealed_target': 'target'})\n    df['target'].fillna(-0.06020069, inplace=True)\n    df = clean_format(df)\n    return df\n\ndef manage_buffer(buffer, df, max_lag):\n\n    # df['iteration'] = df['iteration'].max()  # Ensure the iteration number is consistent in the new df chunk\n    if buffer.empty:\n        return df.copy()\n    elif buffer['iteration'].nunique() < max_lag:\n        return pd.concat([buffer, df], ignore_index=True)\n    else:\n        oldest_iteration = buffer['iteration'].min()\n        buffer = buffer[buffer['iteration'] > oldest_iteration]\n        return pd.concat([buffer, df], ignore_index=True)\n    \n\ndef append_target_lags(df, target_buffer, lags):\n\n    column_to_lag = 'target'\n    df_lagged = df.copy()\n\n    for lag in lags:\n\n        temp_df = target_buffer.copy()\n        temp_df['date_id'] = temp_df['date_id'] + lag\n        \n        temp_df.rename(columns={column_to_lag: f'lag{lag}_{column_to_lag}'}, inplace=True)\n        \n        df_lagged = pd.merge(df_lagged, temp_df[['stock_id', 'date_id', 'seconds_in_bucket', f'lag{lag}_{column_to_lag}']], \n                             on=['stock_id', 'date_id', 'seconds_in_bucket'], \n                             how='left')\n        \n    return df_lagged\n\n\n","metadata":{"papermill":{"duration":0.022966,"end_time":"2023-10-30T00:45:37.057562","exception":false,"start_time":"2023-10-30T00:45:37.034596","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:06:11.476615Z","iopub.execute_input":"2023-12-18T01:06:11.477233Z","iopub.status.idle":"2023-12-18T01:06:11.488113Z","shell.execute_reply.started":"2023-12-18T01:06:11.477199Z","shell.execute_reply":"2023-12-18T01:06:11.487108Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    import gc\n    # remove train_small and train and clear the memory\n    del train\n    gc.collect()","metadata":{"papermill":{"duration":0.229017,"end_time":"2023-10-30T00:45:37.292673","exception":false,"start_time":"2023-10-30T00:45:37.063656","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:06:11.489569Z","iopub.execute_input":"2023-12-18T01:06:11.489981Z","iopub.status.idle":"2023-12-18T01:06:11.870587Z","shell.execute_reply.started":"2023-12-18T01:06:11.489943Z","shell.execute_reply":"2023-12-18T01:06:11.869426Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#@title is inference\nif is_inference:\n\n    if online_learning:\n        \n\n        #rnn\n        online_rnn_lr_rate    = 2e-4\n        online_rnn_batch_size = 16384\n        online_rnn_epochs     = 4\n\n\n\n    if kaggle:\n        import optiver2023\n        env = optiver2023.make_env()\n        iter_test = env.iter_test()\n    else:\n        env = MockApi()\n        iter_test = env.iter_test()\n    \n    if load_models:\n        \n\n\n        if is_rnn:\n\n            print(\"loading rnn models...\")\n            loaded_data          = load_pickle(f'{models_path}rnn_all_data.pkl')\n            rnn_scaler               = loaded_data[\"rnn_scaler\"]\n            rnn_medians              = loaded_data[\"rnn_medians\"]\n            rnn_categorical_features = loaded_data[\"rnn_categorical_features\"]\n            rnn_numerical_features   = loaded_data[\"rnn_numerical_features\"]\n\n            rnn_models = []\n            for i in range(num_models['rnn']):\n                loaded_model = load_model(f\"{models_path}rnn_model_seed_{i}.h5\")\n                rnn_models.append(loaded_model)\n            print(\"Done!\")\n\n\n\n    #---------------------------buffer management ---------------------------\n    max_target_lag  = max(target_lags)\n    i               = 0\n    target_counter  = 0\n    target_buffer   = pd.DataFrame()\n    total_test      = pd.DataFrame()\n    daily_online    = pd.DataFrame()\n    #---------------------------buffer management ---------------------------\n\n\n    for (test, revealed_targets, sample_prediction) in iter_test:\n        \n\n        test = convert_price_cols_float32(test)\n\n\n        if test.seconds_in_bucket.iloc[0] == 0:\n          \n            #----------------------------- storing previous targets -------------------------\n            revealed_targets = clean_up(revealed_targets)\n            revealed_targets['iteration'] = target_counter\n            target_buffer = manage_buffer(target_buffer, revealed_targets, max_target_lag)\n            target_counter += 1\n            if not daily_online.empty and online_learning:\n\n                daily_online.drop(columns='target', inplace=True)\n                daily_online = pd.merge(daily_online, revealed_targets[['stock_id', 'date_id', 'seconds_in_bucket', 'target']], \n                                            on=['stock_id', 'date_id', 'seconds_in_bucket'], \n                                            how='inner')\n\n\n                if is_rnn:\n                    for rnn_model in rnn_models:\n\n                        lr_schedule = ExponentialDecay(\n                            initial_learning_rate=online_rnn_lr_rate,\n                            decay_steps=10000,\n                            decay_rate=0.8,\n                            staircase=True)\n\n                        optimizer = Adam(learning_rate=lr_schedule)\n                        rnn_model.compile(optimizer=optimizer, loss = \"mean_absolute_error\")\n\n                        online_batches, online_targets = online_pass_for_rnn(daily_online, rnn_numerical_features, rnn_categorical_features, window_size)\n                        history = rnn_model.fit(online_batches, online_targets, epochs=online_rnn_epochs, batch_size=online_rnn_batch_size)\n                        print(\"---------------------------------------\")\n\n\n            daily_online = pd.DataFrame()\n            #----------------------------- storing previous targets -------------------------\n\n\n        test['iteration'] = i\n\n\n        test = append_target_lags(test, target_buffer, target_lags)\n        \n\n\n\n        #------------------------ get the full data up to current seconds in bucket in date-----------------------------------\n        if not daily_online.empty:  \n            full_data = pd.concat([daily_online[test.columns], test], ignore_index=True)\n        else:\n            full_data = test\n        #------------------------ get the full data up to current seconds in bucket in date-----------------------------------\n\n        test = full_data.iloc[-test.shape[0]:].copy()\n    \n\n        \n        test['target'] = 0\n\n        if test.currently_scored.iloc[0]:\n\n            if is_rnn:\n                full_data['target'] = 0\n                X_test, _           = second_pass_for_rnn(full_data, rnn_numerical_features, rnn_categorical_features, window_size, is_inference=True)\n                test['target'] += make_predictions(rnn_models, X_test,model='nn').flatten()\n\n\n\n        if not kaggle:\n            total_test = pd.concat([total_test, test], ignore_index=True)\n\n        \n        daily_online = pd.concat([daily_online, test], ignore_index=True)\n\n\n        sample_prediction = pd.merge(sample_prediction.drop(columns='target'), test[['row_id', 'target']], on=['row_id'], how='left')\n        sample_prediction['target'].fillna(0, inplace=True)\n        # sample_prediction['target'].replace([np.inf, -np.inf], 0, inplace=True)\n\n        env.predict(sample_prediction)\n        i += 1","metadata":{"papermill":{"duration":128.751281,"end_time":"2023-10-30T00:47:46.050532","exception":false,"start_time":"2023-10-30T00:45:37.299251","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T01:06:11.873491Z","iopub.execute_input":"2023-12-18T01:06:11.874116Z","iopub.status.idle":"2023-12-18T01:07:15.415861Z","shell.execute_reply.started":"2023-12-18T01:06:11.874046Z","shell.execute_reply":"2023-12-18T01:07:15.414841Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"loading rnn models...\nDone!\nThis version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"},{"name":"stderr","text":"Processing groups: 100%|██████████| 200/200 [00:01<00:00, 138.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n1/1 [==============================] - 6s 6s/step - loss: 5.2829\nEpoch 2/4\n1/1 [==============================] - 0s 391ms/step - loss: 5.2921\nEpoch 3/4\n1/1 [==============================] - 0s 374ms/step - loss: 5.2960\nEpoch 4/4\n1/1 [==============================] - 0s 370ms/step - loss: 5.2993\n---------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing groups: 100%|██████████| 200/200 [00:01<00:00, 135.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n1/1 [==============================] - 6s 6s/step - loss: 5.2940\nEpoch 2/4\n1/1 [==============================] - 0s 400ms/step - loss: 5.2887\nEpoch 3/4\n1/1 [==============================] - 0s 396ms/step - loss: 5.2873\nEpoch 4/4\n1/1 [==============================] - 0s 405ms/step - loss: 5.2814\n---------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing groups: 100%|██████████| 200/200 [00:01<00:00, 137.11it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 6s 6s/step - loss: 5.3230\nEpoch 2/4\n1/1 [==============================] - 0s 400ms/step - loss: 5.2983\nEpoch 3/4\n1/1 [==============================] - 0s 472ms/step - loss: 5.2981\nEpoch 4/4\n1/1 [==============================] - 0s 410ms/step - loss: 5.2979\n---------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing groups: 100%|██████████| 200/200 [00:01<00:00, 135.62it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 6s 6s/step - loss: 5.7834\nEpoch 2/4\n1/1 [==============================] - 0s 432ms/step - loss: 5.7879\nEpoch 3/4\n1/1 [==============================] - 0s 413ms/step - loss: 5.7727\nEpoch 4/4\n1/1 [==============================] - 0s 415ms/step - loss: 5.7890\n---------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing groups: 100%|██████████| 200/200 [00:01<00:00, 136.17it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 6s 6s/step - loss: 5.7662\nEpoch 2/4\n1/1 [==============================] - 0s 399ms/step - loss: 5.7656\nEpoch 3/4\n1/1 [==============================] - 0s 416ms/step - loss: 5.7801\nEpoch 4/4\n1/1 [==============================] - 0s 404ms/step - loss: 5.7805\n---------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing groups: 100%|██████████| 200/200 [00:01<00:00, 134.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n1/1 [==============================] - 6s 6s/step - loss: 5.8051\nEpoch 2/4\n1/1 [==============================] - 0s 422ms/step - loss: 5.7824\nEpoch 3/4\n1/1 [==============================] - 0s 420ms/step - loss: 5.7973\nEpoch 4/4\n1/1 [==============================] - 0s 417ms/step - loss: 5.8053\n---------------------------------------\n","output_type":"stream"}]}]}